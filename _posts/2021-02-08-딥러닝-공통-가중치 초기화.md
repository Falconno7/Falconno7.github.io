---
title: "가중치 초기화 (Weight Initialization)"
excerpt: "가중치 초기값 설정 방법 및 다양한 방법론 소개"
categories:
    - 딥러닝
    - 공통
tags:
    - 가중치
    - Random numbers
    - Xavier
    - He
toc: true
toc_sticky: true
use_math: true
---

## 1. 가중치 초기값이 0이거나 동일한 경우
1. 가중치의 초기값을 모두 0으로 초기화하거나 동일한 값으로 초기화할 경우 모든 뉴런이 동일한 출력값을 내보낼 것입니다. 그렇게 될 경우, 역전파 시 각 뉴런이 모두 동일한 그래디언트 값을 가지게 됩니다. 학습이 잘 되려면, 각 뉴런이 가중치에 따라 비대칭이어야 하는데, 모든 뉴런이 동일한 그래디언트로 가중치 값이 변경되므로 뉴런의 개수가 아무리 많아도 뉴런이 하나 뿐인 것처럼 작동하기 때문에 학습이 제대로 이루어지지 않습니다. 따라서 가중치 초기값을 동일한 값으로 초기화 해서는 안됩니다.

## 2. 작은 난수(Small Random numbers)인 경우.
1. 가중치 초기값은 작은 값으로 초기화 해야하는 데, 그 이유는 활성함수가 sigmoid일 경우 가중치 초기값이 크면 0 ~ 1로 수럼하는데 그래디언트 소실이 발생하게 되기 때문입니다. 또, ReLU일 경우 절대값이 클 경우에는 Dead ReLU 문제가 발생하고, 양수일 때는 그래디언트 폭주가 일어나게 됩니다.
2. 따라서 가중치 초기값을 작게 초기화 해야 하며, 동일한 초기값을 가지지 않도록 랜덤하게 초기화 해야합니다. 일반적으로 가중치 초기값은 평균이 0이고 표준편차가 0.01인 정규분포를 따르는 값으로 랜덤하게 초기화 합니다.
3. 그러나, 이러한 가중치 초기화 방법은 깊은 신경망 모형에서는 문제가 발생하게 됩니다. 예를 들어, 평균이 0이고 표준편차가 0.01인 정규분포를 따르는 값으로 랜덤하게 초기화하고 tanh를 활성화 함수로 사용하였을 경우, 아래 그림처럼 첫번 째 hidden layer를 제외한 나머지 레이어들이 모두 0을 출력합니다.
4. 또한, 평균이 0이고 표준편차가 1인 정규분포를 따르는 값으로 랜덤하게 초기화하고 tanh를 활성화 함수로 사용했을 경우에는 -1, 1에 출력이 집중되면서 그래디언트 소실 문제가 발생하게 된다.
5. 따라서, 작은 난수로 가중치를 초기화 하는 방법 또한 심층 신경망에서는 적합하지 않다고 할 수 있다.<br/>
<br/>
![레이어1만 활성화된 모습](/assets/images/small_random_numbers_tanh.jpg)

```python
%matplotlib inline
import numpy as np
import matplotlib.pyplot as plt


# sigmoid
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# ReLU
def ReLU(x):
    return np.maximum(0, x)

# tanh
def tanh(x):
    return np.tanh(x)

def weight_init(method=None):
    '''가중치 초기화 함수
    
    Args:
        - method: 가중치 초기화 방법(large, small, xavier, relu)
    Returns:
        - np.array형태의 가중치 초기값
    '''
    w = 0
    if method == 'large':
        w = np.random.randn(node_num, node_num) * 1
    elif method == 'small':
        w = np.random.randn(node_num, node_num) * 0.01
    elif method == 'xavier':
        w = np.random.randn(node_num, node_num) * np.sqrt(1.0 / node_num)  # Xavier init
    elif method == 'he':
        w = np.random.randn(node_num, node_num) * np.sqrt(2.0 / node_num)  # He init
    
    return w

input_data = np.random.randn(1000, 100)  # 1000개의 데이터
node_num = 100  # 각 은닉층의 노드(뉴런) 수
hidden_layer_size = 5  # 은닉층이 5개
activations = {}  # 이곳에 활성화 결과를 저장

x = input_data

for i in range(hidden_layer_size):
    if i != 0:
        x = activations[i-1]

    w = weight_init('small')
    a = np.dot(x, w)

    # z = sigmoid(a)
    # z = ReLU(a)
    z = tanh(a)

    activations[i] = z

# 히스토그램 그리기
for i, a in activations.items():
    plt.subplot(1, len(activations), i+1)
    plt.title(str(i+1) + "-layer")
    if i != 0: plt.yticks([], [])
#     plt.xlim(0.1, 1)
    plt.ylim(0, 7000)
    plt.hist(a.flatten(), 30, range=(-1,1))
plt.show()
```
## 3. Xavier(자비에) 초기화와 He(허) 초기화

### 1. Xavir Initialization
$$
평균 = 0,\ \ 표준편차 \sigma = \sqrt{\frac{2}{N_{inputs} + N_{outputs}}}
$$
<br/>
또는,<br/><br/>
$$
r = \sqrt{\frac{2}{N_{inputs} + N_{outputs}}}, 일 \ 때, -r과 \ +r사이의\  균등분포
$$
입력의 연결 개수와 출력의 연결 개수가 비슷할 경우,<br/><br/>
$$
\sigma = \frac{1}{\sqrt{N_{inputs}}} \ \ \ 또는 \ r = \frac{\sqrt{3}}{\sqrt{N_{inputs}}}를 \ 사용
$$

1. Xavier Glort과 Yoshua Bengio는 적절한 데이터가 흐르기 위해서는 각 레이어의 출력에 대한 분산이 입력에 대한 분산과 같아야 하며, 역전파에서 레이어를 통과하기 전과 후의 그래디언트 분산이 동일해야 한다고 주장합니다.

2. Xavier 초기값은 활성함수가 선형이라고 가정합니다. 그래서 sigmoid 계열의 활성화 함수는 좌우 대칭이며, 가운데 부분이 선형이라고 생각합니다.

### 2. He Initialization
1. Xavier 초기값은 ReLU활성화 함수에서는 레이어가 깊어질 수록 출력값이 0으로 치우치는 문제가 발생합니다. Kaiming He는 'Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification'논문에서 ReLU에 적합한 초기값을 제안했는 데, 이를 그의 이름을 따 He 초기값이라고 합니다.
$$
평균이 0이고 표준편차 _sigma = \sqrt{2} * \sqrt{\frac{2}{N_{inputs + N_{outputs}}}}인 \ 정규분포
$$
<br/>
<br/>
또는,
$$
r = \sqrt{2} * \sqrt{\frac{6}{N_{inputs} + N_{outputs}}}, 일 \ 때, -r과 \ +r사이의\  균등분포
$$
<br/>
<br/>
또는,
$$
입력의 \ 연결 \ 개수와 \ 출력의 \ 연결 \ 개수가 \ 비슷할 \ 경우 \ \sigma = \frac{\sqrt{2}}{\sqrt{N_{inputs}}}또는 \ r = \frac{\sqrt{2} * \sqrt{3}}{\sqrt{N_{inputs}}}를 \ 사용
$$
<br/>
<br/>

2. 위 식에서 알 수 있듯이, He 초기값은 Xavier초기값에서 $\sqrt{2}$배 해줬다는 것을 확인할 수 있다. 그 이유는 ReLU는 입력이 음수일 때 출력이 전부 0이기 때문에 더 넓게 분포시키기 위해서이다.

## 4. 어떤 가중치 초기화를 사용하는 것이 좋을까?
1. 가중치 초기화의 경우 Sigmoid일 경우에는 Xavier, ReLU일 경우 He 초기값을 사용하는 것이 좋습니다.

## 5. 느낀점
1. 제가 부족한 부분이 많아서, 완벽하게 이해는 하지 못했습니다. 작성하면서 이해를 돕기 위해, 같이 공부했던 학우들에게 질문하면서 gradient clip등 더 공부해야 할 분야가 많다는 걸 깨달았습니다. 
2. 포스트를 작성하면서 tensorflow의 가중치 default는 정규분포라는 것을 알게 되었습니다. 
3. 고전적인 머신러닝 기법들은 feature extractor를 사람이 만들어야 했습니다.(haar features 처럼) 이 부분을 딥러닝 아키텍처를 이용해서 기계가 대신 찾아주게 된 것이 고전적인 머신러닝 기법과의 차이입니다. 그러면 이 특징이라는 것이 원본에서 나와야 하는데 초기 weight 가중치 때문에 분산이 불규칙해지면 원본의 분산을 찾을 수가 없게 됩니다. 그래서 원본 분산을 유지하면서 넘기기 위해 초기 weight설정이 he, xavier처럼 수식이 만들어 집니다. 즉, Normal distribution이 안좋다는 것이 아니라, 어떤 데이터에서는 좋을 수도 있습니다. He나 Xavier도 결국 Norm(0, x)이니까요. 결론적으로는 weight의 초기 분산이 너무 크면 양극화가 되기 때문에 He나 Xavier가 탄생하게 된 것 같습니다. 