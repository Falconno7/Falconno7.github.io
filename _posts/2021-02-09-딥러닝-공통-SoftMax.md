---
title: "Soft Max"
excerpt: "출력계층에 대한 기본적인 내용을 다룹니다."
categories:
    - 딥러닝
    - 공통
tags:
    - 출력계층
    - SoftMax
    - 항등 함수
toc: true
toc_sticky: true
use_math: true
---

## 1. 출력층 설계하기
1. 머신러닝은 분류(classification)와 회귀(regression)로 나뉩니다. 일반적으로 회귀에는 항등 함수를, 분류에는 소프트맥스 함수를 사용합니다.

2. 항등 함수(identity function)는 입력을 그대로 출력합니다. 입력과 출력이 항상 같다는 뜻의 항등입니다.

## 2. Soft Max 함수
$$
y_{k} = \frac{exp(a_k)}{\sum_{i=1}^n exp(a_i)}
$$
<br/>
<br/>
1. n은 출력층의 뉴런 수, y_k는 그 중 k번째 출력임을 뜻합니다.
2. 소프트맥스 함수의 분자는 입력 신호 $a_k$의 지수 함수, 분모는 모든 입력 신호의 지수 함수의 합입니다.

### 1. Soft Max함수 사용시 주의점
1. SoftMax는 일반적으로 구현하면 오버플로 문제가 생깁니다. 그 이유는 지수 함수를 사용하기 때문입니다. 지수 함수는 아주 큰 값을 내뱉게 되는데, 이런 큰 값끼리 나눗셈을 하면 결과 수치가 이상해집니다.
2. 개선된 SoftMax<br/><br/>
$$
y_k = \frac{exp(a_k)}{\sum_{i=1}^nexp(a_i)} = \frac{Cexp(a_k)}{C\sum_{i=1}^nexp(a_i)}
$$
<br/>
<br/>
$$
=\frac{exp(a_k + logC)}{\sum_{i=1}^nexp(a_i + logC)}
$$
<br/>
<br/>
$$
=\frac{exp(a_k + logC')}{\sum_{i=1}^nexp(a_i + C')}
$$
<br/>
<br/>
    1. 개선된 SoftMax는 C라는 임의의 정수를 분자와 분모 양쪽에 곱했습니다. 그 다음으로 C를 지수 함수 exp() 안으로 옮겨 logC로 만들었습니다. 마지막으로 logC를 C'라는 새로운 기호로 바꿉니다.
    2. 이 식이 말하는 것은 소프트맥스의 지수 함수를 계산할 때 어떤 정수를 더해도 결과는 바뀌지 않는다는 것입니다. 여기서 C'에 어떤 값을 대입해도 상관없지만, 오버플로를 막는 목적으로는 입력 신호 중 최댓값을 이용하는 것이 일반적입니다.

### 2. SoftMax함수 특징
1. SoftMax 함수 출력 값은 0에서 1.0 사이의 실수입니다. 또, 소프트맥스 함수 출력의 총 합은 1입니다. 이 성질 덕분에 소프트맥스 함수의 출력을 확률로 해석할 수 있습니다.

2. 신경망 분류에서는 일반적으로 가장 큰 출력을 내는 뉴런에 해당하는 클래스로만 인식합니다. 그리고 소프트맥스 함수를 적용해도 출력이 가장 큰 뉴런의 위치는 달라지지 않습니다. 결과적으로 신경망으로 분류할 때는 출력층의 소프트맥스 함수를 생략해도 됩니다. 현업에서도 지수 함수 계산에 드는 자원낭비를 줄이고자 출력층의 소프트맥스 함수는 생략하는 것이 일반적입니다.