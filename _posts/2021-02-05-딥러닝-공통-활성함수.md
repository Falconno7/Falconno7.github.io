---
title: "활성함수 (ReLU, sigmoid, tanh, leakyReLU)"
excerpt: "딥러닝에서 사용되는 활성함수의 설명과 장단점"
categories:
    - 딥러닝
    - 공통
tags:
    - 활성함수
    - ReLU
    - sigmoid
    - tanh
    - leakyReLU
toc: true
toc_sticky: true
use_math: true
---

## 1. Sigmoid<br/>
$$ y = \frac{1}{1+exp(-x)}$$


### 1. 시그모이드 함수 특징
1. 비선형 함수.
    1. 비선형 함수는 문자 그대로 '선형이 아닌'함수 입니다. 즉, 직선 1개로 그릴 수 없는 함수를 그릴 수 없는 함수를 말합니다.<br/>
    2. 신경망에서는 활성함수로 비선형함수를 사용해야 합니다. 예를 들어 h(x) = cx를 활성함수로 사용한 3층 네트워크를 떠올려보세요. 이를 식으로 나타내면 $y(x) = h(h(h(x)))$가 됩니다. 이 계산은 $y(x) = ax$와 똑같은 식입니다. a = c<sup>3</sup>라고만 쓰면 되는 것이죠. 그래서 층을 쌓는 혜택을 얻고 싶다면 활성화 함수로는 반드시 비선형 함수를 사용해야 합니다.<br/>
2. Vanishing Gradient 문제 원인이 되는 함수 입니다. sigmoid는 0 ~ 1사이의 값을 가지기 때문에, 딥러닝의 깊이가 깊어질 수록 역전파의 갱신값은 점점 작아져서 소실되게 됩니다. 또한, sigmoid 자체의 계산량이 크기 때문에 학습시간도 오래걸린다는 단점이 있습니다. 보통은 이 문제를 해소하기 위해 ReLU함수를 대신 사용합니다.

### 2. 설명
1. "칵스"라는 사람이 sigmoid함수를 만들었습니다.

2. sigmoid 함수는 기본적으로 odds비에서 로그를 취해준 logits에서 파생된 함수입니다. odds비는 사건의 성공확률 / 실패확률을 나타내는 수식이며, 식은 y = "성공확률" 일 때, $ \frac{y}{1-y} $
로 나타냅니다.
3. odds비를 사용하지 않고, 로그를 취해 sigmoid함수를 취하는 이유는 활성함수의 출력값이 0 ~ 1사이가 되어야 하기 때문입니다. odds비는<br/> 
$$ odds = \frac{y}{1-y}$$ 
<br/>
$$ if:y = 1 일 때, answer = \infin $$
<br/>
$$ elif : y = 0 일 때, answer = 0$$
<br/>
이 되어버립니다.
4. 출력값이 0 ~ 1사이가 되어야 하는 이유는 다음과 같습니다. odds는 사건확률 / 사건이 일어나지 않을 확률 이기 때문에 1개의 사건이 일어나지 않았을 때의 성공의 기대값이라고 생각이 가능합니다. 그렇기 때문에, odds가 4 즉 4/1일 때, 4번의 사건이 일어나면 1번은 반대에 해당하는 즉, 실패사건이 일어나게 됩니다. 만약, y가 1에 가까우면, 무한번 시도해야 1번의 실패가 일어나는 것이어야 하고, y가 0에 수렴하면, -무한번(존재하지 않는 수)를 시도해야 합니다.
5. 이러한 단점을 보완하기 위해 자연로그를 취하게 됩니다. 자연로그 0은 무한대의 값을 가지며, e는 무리수이면서 초월수이기에 자기자신조차 나눠떨어지지 않으며 odds에 자연로그를 취해주기 적합합니다.
6. 우리는 평소, y의 값을 도출하기 위해 y = ax + b와 같은 선형식을 써왔습니다. a를 weight의 첫글자인 w로 대신하고, odds에 log를 취해준 값과 식을 합치게 되면, 다음과 같은 수식이 탄생합니다. 
$$ln(\frac{y}{1-y}) = wx + b$$

### 3. 증명
1. 위에서 설명한 $ ln(\frac{y}{1-y}) = wx + b$을 전개해서, sigmoid함수로 만들어 보겠습니다.
$$
ln(\frac{y}{1-y}) = wx + b
$$
$$
 e^{wx+b} = \frac{y}{1-y}
$$
$$
  \frac{1}{e^{wx+b}} = \frac{1-y}{y} = \frac{1}{y} - 1
$$
$$
 1 + \frac{1}{e^{wx+b}} = \frac{1}{y}
$$
$$
\frac{e^{wx+b}}{e^{wx+b}} +\frac{1}{e^{wx+b}} = \frac{1}{y}
$$
$$
\frac{1 + e^{wx+b}}{e^{wx+b}} = \frac{1}{y}
$$
$$
\frac{e^{wx+b}}{1 + e^{wx+b}} = y 
$$
$$
\frac{\frac{1}{e^{wx+b}}}{\frac{1}{e^{wx+b}}} * \frac{e^{wx+b}}{1 + e^{wx+b}} = y 
$$
$$
\frac{1}{\frac{1 + e^{wx+b}}{e^{wx+b}}} = y
$$
$$
\frac{1}{\frac{1}{e^{wx+b} + 1}} = y
$$
$$
\frac{1}{1+e^{-wx+b}} = y
$$
<br/>
2. 딥러닝에서는 학습을 위해 Back-propagation을 사용합니다. 이 때 sigmoid를 미분하는 과정이 필요하게 되므로, 이번 증명에서 미분까지 진행하도록 하겠습니다. y = wx + b를 편의상 g(x)로 치환하겠습니다.
sigmoid또한, S(x)로 부분적으로 사용하겠습니다.
$$
\partial s(g(x)) = \partial (\frac{1}{1+e^{-g(x)}})
$$
$$
=\partial (1+e^{-g(x)})^{-1}
$$
$$
=-(1+e^{-g(x)})^{-2} * \partial (1+e^{-g(x)})
$$
$$
=-(1+e^{-g(x)})^{-2} * e^{-g(x)}-1
$$
$$
=\frac{e^{-g(x)}}{(1+e^{-g(x)})^2}
$$
$$
=\frac{1+e^{-g(x)}-1}{(1+e^{-g(x)})^2}
$$
$$
\frac{-1}{(1+e^{-g(x)})}
$$
$$
=\frac{1}{(1+e^{-g(x)})} - \frac{1}{(1+e^{-g(x)})^2}
$$
$$
= \frac{1}{1+e^{-g(x)}} * (1 - \frac{1}{1+e^{-g(x)}})
$$
$$
=S(g(x)) * (1 - S(g(x)))
$$
---
## 2. ReLU 함수
$$
ReLU(x) = max(0,x) if(x <= 0) = 0, else(x > 0) = x
$$
###1. ReLU 함수 특징
1. Sigmoid 함수와 비교해보면 계산량이 적은 것을 알 수 있습니다.
2. ReLU는 출력값이 우리가 원하는 0 ~ 1사이 값이 아니기 때문에, 맨 마지막 레이어는 Sigmoid 함수를 사용해야 합니다.

---
## 3. leakyReLU 함수
$$
LeakyReLU_\alpha(x) = max(\alpha x, x)
$$
### 1. ReLU의 단점을 보완하기 위한 leakyReLU
1. 
